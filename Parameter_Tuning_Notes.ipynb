{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ffeb4d",
   "metadata": {},
   "source": [
    "# Loss Functions to be Explored\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_\\text{Weighted Cross-Entropy} &= - \\sum_i \\left[w_+ b_i \\log p_i + w_- (1 - b_i) \\log (1 - p_i)\\right] = - w_+ \\mathbf{b} \\cdot \\log \\mathbf{p} + w_- \\bar{\\mathbf{b}} \\cdot \\log \\bar{\\mathbf{p}}\\\\\n",
    "\n",
    "\\mathcal{L}_\\text{Dice} &= 1 - \\frac{2\\mathbf{p} \\cdot \\mathbf {b}}{\\sum_i p_i + \\sum_i b_i}\\\\\n",
    "\n",
    "\\mathcal{L}_\\text{Focal-Tversky} &= \\left( 1 - \\frac{2\\mathbf{p}\\cdot\\mathbf{b}}{\\mathbf{p}\\cdot\\mathbf{b} + \\beta \\bar{\\mathbf{p}} \\cdot \\mathbf{b} + (1 - \\beta) \\mathbf{p} \\cdot \\bar{\\mathbf{b}}} \\right)^\\gamma.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{p}$ are the probabilities (softmaxxed outputs along channel dimension) from the model, and $\\mathbf{b}$ are the binary mask (0, 1) values. We use $\\bar{\\cdot}$ to represent the complement, i.e. $\\bar{x_i} = 1 - x_i$ in index notation.\n",
    "\n",
    "There are actually many variations of the Dice loss, with a couple listed below,\n",
    "$$\n",
    "\\mathcal{L}_\\text{Dice} =\n",
    "\\begin{cases}\n",
    "    &1 - \\frac{2\\mathbf{p} \\cdot \\mathbf {b}}{\\sum_i p_i + \\sum_i b_i},\\\\\n",
    "    \\\\\n",
    "    &1 - \\frac{2\\mathbf{p} \\cdot \\mathbf {b}}{\\lvert p \\rvert _2^2 + \\lvert b \\rvert _2 ^2}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "# Model Selection\n",
    "\n",
    "Loss Function: $\\mathcal{L}_\\text{Weighted Cross-Entropy} + \\alpha \\mathcal{L}_\\text{Focal-Tversky}$,\n",
    "and model parameters as listed: $\\alpha = 1,\\; \\beta = 0.7,\\; \\gamma = 0.75,\\; (w_-, w_+) = (0.1, 0.9)$.\n",
    "\n",
    "Only tackling 2D mode, since FCN-ResNets can only handle 2D data.\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "\n",
    "- FCN-ResNet50 had peak Val IoU of ~55% requiring ~20 epochs.\n",
    "- FCN-ResNet101 had peak Val IoU of ~63% requiring ~60 epochs.\n",
    "- U-Net had peak Val IoU of ~70% requiring 5 epochs.\n",
    "\n",
    "Selected U-Net.\n",
    "\n",
    "# Loss Selection\n",
    "\n",
    "## $\\mathcal{L}_\\text{Weighted Cross-Entropy} + \\alpha \\mathcal{L}_\\text{Focal-Tversky}$\n",
    "\n",
    "$\\alpha = 1,\\; \\beta = 0.7,\\; \\gamma = 0.75,\\; (w_-, w_+) = (0.1, 0.9)$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- Unstable training and exploding gradients. This was not an issue during model selection and I do not believe I made any changes that should have impacted training stability. Nevertheless, we look to tackle issues as they arise.\n",
    "- Trained well before running into NaNs. 2D U-Net had peak Val IoU of ~50% and 3D U-Net had peak Val IoU of ~79.3%.\n",
    "Note that segmentations looked near perfect on evaluation, with 2D U-Net only doing badly on Carotid.\n",
    "\n",
    "## $\\mathcal{L}_\\text{Weighted Cross-Entropy}$\n",
    "\n",
    "$(w_-, w_+) = (0.1, 0.9)$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- No exploding gradients\n",
    "- 2D U-Net 60% Val IoU in 2 epochs\n",
    "- 3D U-Net 76% Val IoU in 5 epochs\n",
    "\n",
    "$(w_-, w_+) = (0.2, 0.8)$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- \n",
    "- 2D U-Net 61% Val IoU in 1 epoch\n",
    "- 3D U-Net 79% Val IoU in 5 epochs\n",
    "\n",
    "$(w_-, w_+) = (0.3, 0.7)$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- No exploding gradients, and better performance than (0.1, 0.9)\n",
    "- 2D U-Net 59% Val IoU in 3 epochs\n",
    "- 3D U-Net 81% Val IoU in 7 epochs\n",
    "\n",
    "## $\\mathcal{L}_\\text{Dice}$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- No exploding gradients\n",
    "- 2D U-Net 66% Val IoU in 3 epochs\n",
    "- 3D U-Net 82% Val IoU in 5 epochs\n",
    "\n",
    "## $\\mathcal{L}_\\text{Focal-Tversky}$\n",
    "$\\beta = 0.7,\\; \\gamma = 0.75$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- Experienced exploding gradients for both 2D and 3D cases.\n",
    "- 2D U-Net 49% Val IoU in 1 epoch\n",
    "- 3D U-Net 79% Val IoU in 1 epoch\n",
    "- I suspect exploding gradients to be attributed to the Focal-Tversky Loss function.\n",
    "\n",
    "## $\\mathcal{L}_\\text{Weighted Cross-Entropy} + \\alpha \\mathcal{L}_\\text{Dice}$\n",
    "\n",
    "$\\alpha = 1., (w_-, w_+) = (0.1, 0.9)$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- No exploding gradients\n",
    "- 2D U-Net 63% Val IoU in 8 epochs\n",
    "- 3D U-Net 80% Val IoU in 8 epochs\n",
    "\n",
    "$\\alpha = 1., (w_-, w_+) = (0.3, 0.7)$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- No exploding gradients\n",
    "- 2D U-Net 65% Val IoU in 1 epoch\n",
    "- 3D U-Net 81% Val IoU in 3 epochs\n",
    "\n",
    "$\\alpha = 1.5, (w_-, w_+) = (0.1, 0.9)$\n",
    "\n",
    "- Training set: 1000 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- No exploding gradients\n",
    "- 2D U-Net\n",
    "- 3D U-Net 83% Val IoU in 10 epochs\n",
    "\n",
    "Selected DICE Loss\n",
    "\n",
    "# Dataset Trials\n",
    "\n",
    "## Test 1 -- Reducing training set size\n",
    "- Training set: 100 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data\n",
    "- 2D U-Net 68% Val IoU in 29 epochs\n",
    "- 3D U-Net 82% Val IoU in 28 epochs\n",
    "- While the epoch count has shot up dramatically, it is important to note that times have greatly decreased, from ~30 mins to ~9mins on the same set-up, i.e. a 3x speed increase.\n",
    "\n",
    "## Test 2 -- Padding validation set with synthetic data + 20% validation set data split\n",
    "- Training set: 100 pieces of synthetic data\n",
    "- Validation set: 3 pieces of real data + 20 pieces of synthetic data\n",
    "- 2D U-Net 84% Val IoU in 33 epochs\n",
    "- 3D U-Net 90% Val IoU in 42 epochs\n",
    "- Note that Val IoU is no longer a super good metric if there is synthetic data in the validation pool.\n",
    "- Observing evals on real data looks good. 2D U-Net performs badly on carotid, not picking up one of the vessels.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
